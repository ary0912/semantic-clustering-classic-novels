# ğŸ“˜ Word Similarity Clustering from Literature â€” NLP Project

![Typing Header](https://readme-typing-svg.demolab.com?font=Fira+Code&size=24&pause=1000&color=F59E0B&center=true&vCenter=true&width=850&lines=ğŸ“š+Semantic+Clustering+From+Classic+Novels;ğŸ§ +Nouns+as+Nodes+in+a+Meaning+Graph)

<p align="center">
  <img src="https://img.shields.io/badge/NLP-Graph%20Clustering-blue?style=for-the-badge&logo=python"/>
  <img src="https://img.shields.io/badge/Text%20Analysis-Gutenberg-green?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/MDS-KMeans-orange?style=for-the-badge"/>
</p>

---

## ğŸ§  Overview

This project clusters the top 100 most frequent **nouns** from six literary works using two semantic similarity methods:

- ğŸ“ **Graph-Based Similarity**: co-occurrence graph + Dijkstra distance
- ğŸ§¬ **Embedding-Based Similarity**: Word2Vec + t-SNE projection

The goal is to discover latent meaning structures in classical literature.

---

## ğŸ“˜ Corpus

Classic English novels from [Project Gutenberg](https://www.gutenberg.org/):

- *Frankenstein*
- *Twenty Thousand Leagues Under the Sea*
- *Pride and Prejudice*
- *The Adventures of Sherlock Holmes*
- *Moby Dick*
- *Jack Pumpkinhead of Oz*

> ğŸ“ ~30,000 sentences | ğŸ“š ~2.5M words

---

## ğŸ” Methodology

### ğŸ”¤ Preprocessing
- Cleaned text, tokenized with NLTK
- Extracted top 100 nouns via POS tagging

### ğŸ•¸ï¸ Graph-Based Clustering
- Created co-occurrence graph of nouns (100Ã—100)
- Edge weights = 1 / co-occurrence count (to form distance)
- Used **Dijkstraâ€™s algorithm** to compute shortest paths
- Applied **MDS** for dimensionality reduction
- Clustered with **KMeans (k=5)**

### ğŸ§­ Embedding-Based Clustering
- Trained **Word2Vec** on the corpus
- Applied **t-SNE** for visualization
- Clustered using **KMeans**

---

## ğŸ“Š Results

| Method           | Silhouette Score |
|------------------|------------------|
| Graph Distance   | â­ **0.479**      |
| Word2Vec (t-SNE) | 0.316            |

âœ… **Final Insight**:  
Graph-based clustering using co-occurrence + Dijkstra outperforms embedding-based methods for this task.

ğŸ§  **What It Means**:  
Literal co-presence in literary context reveals deeper thematic groupings like:
- ğŸ‘¤ Characters (man, captain, woman)
- ğŸ  Locations (room, street, sea)
- ğŸ”§ Objects (boat, gun, window)
- â¤ï¸ Emotions (love, fear, truth)

---

## âš™ï¸ Tech Stack

| Component          | Tools & Libraries            |
|--------------------|------------------------------|
| Language           | Python 3.10                  |
| Preprocessing      | NLTK                         |
| Graph Analysis     | NetworkX                     |
| Clustering         | KMeans (sklearn)             |
| Dim. Reduction     | MDS, t-SNE                   |
| Visualization      | Matplotlib, Seaborn          |

---

## ğŸš€ Quick Start

```bash
# Clone the repo
git clone https://github.com/yourusername/word-graph-nlp.git
cd word-graph-nlp

# Launch notebook
jupyter notebook Final_Code.ipynb
```
