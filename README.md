# ğŸ“š Word Similarity Clustering from Literature  
**Discovering hidden meaning structures in classic novels using NLP, Graph Theory & Word Embeddings**  

<p align="center">
  <img src="https://img.shields.io/badge/Domain-NLP-blue?style=for-the-badge&logo=python"/>
  <img src="https://img.shields.io/badge/Approach-Graph%20%26%20Embedding-orange?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Tech-Python%20%7C%20NLTK%20%7C%20sklearn%20%7C%20NetworkX-green?style=for-the-badge"/>
</p>

---

## ğŸš€ Executive Summary  
In this project, I applied **two semantic similarity methods** to cluster the top 100 most frequent nouns from **six classic novels**.  
The goal: **Reveal hidden thematic structures** such as characters, locations, objects, and emotions â€” without manual labeling.  

**Key Result:**  
ğŸ“Š **Graph-based co-occurrence + Dijkstraâ€™s algorithm outperformed Word2Vec embeddings** for thematic clustering in this dataset.  

---

## ğŸ¯ Why This Matters to a Recruiter
- **Real-world relevance:** Shows ability to work with **messy, large-scale text data** and extract structured insights.  
- **Multi-technique approach:** Combines **graph theory** + **unsupervised ML** + **NLP preprocessing**.  
- **End-to-end pipeline:** From raw text acquisition to final visual clusters.  
- **Measurable outcome:** Compared algorithms using **Silhouette Score** for objective evaluation.  

---

## ğŸ“‚ Dataset
**Source:** [Project Gutenberg](https://www.gutenberg.org/)  
**Works Used (~2.5M words, ~30k sentences):**
- *Frankenstein*
- *Twenty Thousand Leagues Under the Sea*
- *Pride and Prejudice*
- *The Adventures of Sherlock Holmes*
- *Moby Dick*
- *Jack Pumpkinhead of Oz*

---

## ğŸ›  Methodology Overview

| Step | Description | Tools |
|------|-------------|-------|
| **1. Preprocessing** | Text cleaning, tokenization, POS tagging to extract top 100 nouns | NLTK |
| **2. Graph-Based Similarity** | Build co-occurrence graph â†’ Edge weight = 1 / frequency â†’ Dijkstra shortest paths â†’ MDS reduction â†’ KMeans clustering | NetworkX, sklearn |
| **3. Embedding-Based Similarity** | Train Word2Vec â†’ t-SNE reduction â†’ KMeans clustering | Gensim, sklearn |
| **4. Evaluation** | Compare clustering with Silhouette Score | sklearn |
| **5. Visualization** | 2D plots of clusters for interpretability | Matplotlib, Seaborn |

---

## ğŸ“Š Results at a Glance

| Method                 | Silhouette Score | Insight |
|------------------------|------------------|---------|
| **Graph Distance (MDS)** | â­ **0.479**     | Strong thematic grouping |
| **Word2Vec (t-SNE)**    | 0.316            | Weaker separation |

**Themes Identified**:  
- ğŸ‘¤ Characters: *man, captain, woman*  
- ğŸ  Locations: *room, street, sea*  
- ğŸ”§ Objects: *boat, gun, window*  
- â¤ï¸ Emotions: *love, fear, truth*

---

## ğŸ§  Key Takeaways
- **Graph-based methods excel in small, domain-specific corpora** where co-occurrence patterns are rich.
- Embeddings can underperform without very large, diverse training data.
- Combining linguistic intuition with algorithmic modeling yields stronger insights.

---

## âš™ï¸ Tech Stack
**Languages:** Python 3.10  
**Libraries:** NLTK, NetworkX, scikit-learn, gensim, matplotlib, seaborn  
**Concepts Applied:**  
- NLP preprocessing & POS tagging  
- Graph construction & shortest path algorithms  
- Dimensionality reduction (MDS, t-SNE)  
- Clustering & evaluation metrics  

---

## ğŸ–¥ Quick Start
```bash
# Clone repository
git clone https://github.com/yourusername/word-graph-nlp.git
cd word-graph-nlp
```

# Launch analysis
jupyter notebook Final_Code.ipynb

## ğŸ“Œ Impact Statement
This project demonstrates my ability to **design, implement, and evaluate** a complete NLP pipeline â€” leveraging both **graph algorithms** and **machine learning** to solve an open-ended semantic problem. It highlights skills in **data wrangling, algorithm selection, and results interpretation** â€” all critical in production-grade AI systems.

